{"cells":[{"cell_type":"markdown","metadata":{"id":"Bfg0JmQ6JybV"},"source":["# 1. Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kXOqGALPZybe","executionInfo":{"status":"ok","timestamp":1719232004991,"user_tz":-420,"elapsed":29409,"user":{"displayName":"James Valix","userId":"10011400396215233588"}},"outputId":"3d9914e2-f33f-4bfd-eb6d-fc553b7baea7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.2)\n","Collecting pmdarima\n","  Downloading pmdarima-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: yellowbrick in /usr/local/lib/python3.10/dist-packages (1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n","Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.11.4)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (0.5.6)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.4.2)\n","Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (3.0.10)\n","Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.2.2)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (2.0.7)\n","Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (67.7.2)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->pmdarima) (3.5.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.6.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n","Installing collected packages: pmdarima\n","Successfully installed pmdarima-2.0.4\n"]}],"source":["import random\n","random.seed(8)\n","\n","# Import libraries\n","!pip install pandas matplotlib statsmodels pmdarima keras tensorflow yellowbrick\n","\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import pmdarima as pm\n","import math\n","import os\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from math import sqrt\n","from statsmodels.tsa.stattools import adfuller\n","from statsmodels.tsa.arima.model import ARIMA\n","from statsmodels.tsa.statespace.sarimax import SARIMAX\n","from statsmodels.stats.diagnostic import het_arch\n","from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n","from pylab import rcParams\n","\n","from pmdarima import auto_arima\n","from statsmodels.tsa.seasonal import seasonal_decompose\n","\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense, Dropout, Bidirectional\n","\n","import tensorflow as tf\n","from tensorflow.python.util import deprecation\n","deprecation._PRINT_DEPRECATION_WARNINGS = False # to hide warnings during training stage\n","\n","#Import libraries for kmeans clustering model\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from yellowbrick.cluster import KElbowVisualizer\n","from sklearn.cluster import KMeans\n","from sklearn.cluster import AgglomerativeClustering\n","from matplotlib.colors import ListedColormap\n","from sklearn import metrics\n","\n","# Define a customized palette for future plots\n","pal = [\"#682F2F\",\"#B9C0C9\", \"#9F8A78\",\"#F3AB60\", \"#D6B0CA\"]\n","cmap = ListedColormap(pal)\n","palette = pal\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qcF4d6fKwW56","executionInfo":{"status":"ok","timestamp":1719232039400,"user_tz":-420,"elapsed":34412,"user":{"displayName":"James Valix","userId":"10011400396215233588"}},"outputId":"a9b255b1-77ae-4508-b78c-0b969a9d8677"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gyW0NXpnaGyB"},"outputs":[],"source":["# Import dataset\n","data = pd.read_csv('/content/drive/MyDrive/[H4TF DA] Round 2_MLTada/100000 Sales Records.csv')"]},{"cell_type":"markdown","metadata":{"id":"zmbuoFXKKXze"},"source":["# 2. EDA"]},{"cell_type":"markdown","metadata":{"id":"sxpiu-I2KgWu"},"source":["The data set includes 14 data fields, detailed below:"]},{"cell_type":"markdown","metadata":{"id":"JKDmIjn5Kf9W"},"source":["| Feature               | Data Type | Description                                                      |\n","|:----------------------|:----------|:-----------------------------------------------------------------|\n","| Region                | object    | Geographic region of the sale                                    |\n","| Country               | object    | Country where the sale occurred                                  |\n","| Item Type             | object    | Category of the item sold                                        |\n","| Sales Channel         | object    | Channel through which the sale was made (Online/Offline)         |\n","| Order Priority        | object    | Priority level of the order (L/M/H/C)                            |\n","| Order Date            | datetime  | Date when the order was placed                                   |\n","| Order ID              | int       | Unique identifier for the order                                  |\n","| Ship Date             | datetime  | Date when the order was shipped                                  |\n","| Units Sold            | int       | Number of units sold                                             |\n","| Unit Price            | float     | Price per unit of the item                                       |\n","| Unit Cost             | float     | Cost per unit of the item                                        |\n","| Total Revenue         | float     | Total revenue from the sale                                      |\n","| Total Cost            | float     | Total cost incurred for the sale                                 |\n","| Total Profit          | float     | Total profit from the sale                                       |\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_udVlnHSKnwh"},"outputs":[],"source":["# Check the first 5 rows of the data set\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3eYQuNbubYpJ"},"outputs":[],"source":["# Check the data type of data fields\n","data.info()"]},{"cell_type":"markdown","metadata":{"id":"Qfcg8MNVMJ0W"},"source":["However, at the time the data is imported into the ipynb file, the data fields still have the datatype object. Therefore, the team will clean the data before proceeding with further processing."]},{"cell_type":"markdown","metadata":{"id":"J-E6GuTTOzAS"},"source":["## 2.1. Cleaning Data"]},{"cell_type":"markdown","metadata":{"id":"mSARtyjaPI_9"},"source":["The data cleaning process includes converting the datatype of the columns to the appropriate types."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MApjICaXalgh"},"outputs":[],"source":["# Change order date, ship date to datetime\n","data['Order Date']=pd.to_datetime(data['Order Date'])\n","data['Ship Date']=pd.to_datetime(data['Ship Date'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ybtFbtGseI5d"},"outputs":[],"source":["# Define the desired column order\n","new_order = ['Order ID', 'Item Type', 'Sales Channel', 'Order Priority', 'Country', 'Region', 'Order Date', 'Ship Date', 'Units Sold', 'Unit Price', 'Unit Cost', 'Total Revenue', 'Total Cost', 'Total Profit']\n","# Reorder the DataFrame\n","data = data[new_order]\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWRAM7iBei2D"},"outputs":[],"source":["# Gross profit margins\n","data['Gross Profit Margin'] = data['Total Profit'] / data['Total Revenue']*100\n","# Return on investment\n","data['ROI']= data['Total Profit'] / data['Total Cost']*100\n","\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iN2EbCiIfFkB"},"outputs":[],"source":["#Round column Gross Profit Margin and ROI\n","data['Gross Profit Margin']=data['Gross Profit Margin'].round(2)\n","data['ROI']=data['ROI'].round(2)\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kdbinfkcjQip"},"outputs":[],"source":["#Order by Order Date\n","data=data.sort_values(by='Order Date')\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z8tbX-TljQ1-"},"outputs":[],"source":["# Box plot to check outlier\n","data['Year'] = data['Order Date'].dt.year\n","sns.boxplot(x='Year', y='Total Revenue', data=data)\n","plt.title('Total Revenue by Year')\n","plt.xlabel('Year')\n","plt.ylabel('Total Revenue')\n","plt.show()\n","# Count outliers each year\n"]},{"cell_type":"markdown","source":["As can be seen from the plot above, there's a considerable amount of outliers in terms of yearly revenue. For this reason, we decided to filter these outliers from our dataset, determining the existence of these outliers as 'Agents' which poses unnecessary impacts on our predictions."],"metadata":{"id":"Jpgk36wXco_W"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cquOvTcsjaJ2"},"outputs":[],"source":["'''DISPLAY OUTLIER OF 'TOTAL REVENUE' '''\n","def display_outliers(data):\n","  grouped = data.groupby('Year')['Total Revenue'].describe()\n","  grouped['IQR'] = grouped['75%'] - grouped['25%']\n","  grouped['Lower Bound'] = grouped['25%'] - 1.5 * grouped['IQR']\n","  grouped['Upper Bound'] = grouped['75%'] + 1.5 * grouped['IQR']\n","\n","\n","  outliers_count = []\n","  for year in data['Year'].unique():\n","      year_data = data[data['Year'] == year]\n","      outliers = year_data[(year_data['Total Revenue'] < grouped.loc[year, 'Lower Bound']) |\n","                          (year_data['Total Revenue'] > grouped.loc[year, 'Upper Bound'])]\n","      outliers_count.append((year, len(outliers)))\n","\n","  for year, count in outliers_count:\n","      print(f\"Number of outliers in {year}: {count}\")\n","\n","'''REMOVE OUTLIERS'''\n","def remove_rev_outliers(data):\n","\n","    grouped = data.groupby('Year')['Total Revenue'].describe()\n","    grouped['IQR'] = grouped['75%'] - grouped['25%']\n","    grouped['Lower Bound'] = grouped['25%'] - 1.5 * grouped['IQR']\n","    grouped['Upper Bound'] = grouped['75%'] + 1.5 * grouped['IQR']\n","\n","    filtered_data = pd.DataFrame()\n","    for year in data['Year'].unique():\n","        year_data = data[data['Year'] == year]\n","        valid_data = year_data[year_data['Total Revenue'].between(grouped.loc[year, 'Lower Bound'], grouped.loc[year, 'Upper Bound'], inclusive=\"both\")]\n","        filtered_data = pd.concat([filtered_data, valid_data])\n","\n","    return filtered_data\n","\n","'''RECURSIVE FUNCTION TO REMOVE ALL OUTLIERS'''\n","def remove_all_outliers(data):\n","    while True:\n","        new_data = remove_rev_outliers(data)\n","        if len(new_data) == len(data):  # No outliers removed in this iteration\n","            break\n","        data = new_data\n","\n","    return data\n","\n","df = remove_all_outliers(data)\n","display_outliers(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qhYpk56cjdMN"},"outputs":[],"source":["def extract_month(month_year):\n","        return pd.to_datetime(month_year, format='%Y-%m').month\n","\n","df['Month-Year'] = df['Order Date'].dt.strftime('%Y-%m')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MpHLKIixkAn9"},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yBI1AHxZj7Vb"},"outputs":[],"source":["# Drop ID, Order and Ship Date (irrelevant)\n","drops = ['Order ID', 'Order Date', 'Ship Date']\n","df = df.drop(drops, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"eFEN-QLFPXLP"},"source":["## 2.2. Exploring Dataset and Explanation"]},{"cell_type":"markdown","metadata":{"id":"UwUha3DFQTOl"},"source":["### 2.2.1. Check the quality of the dataset"]},{"cell_type":"markdown","metadata":{"id":"e7IdRd3rQrOo"},"source":["The dataset does not include null values, does not possess duplicate values, and the date column is completely unique. From there, in terms of quality, the data set has no problem."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yOfSMCUbQdYW"},"outputs":[],"source":["# Check the number of rows and columns of the data set\n","df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YiYBmoiobFYO"},"outputs":[],"source":["# Check the number of null observations\n","print(df.isnull().sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bbmy3ZCGMq4w"},"outputs":[],"source":["# Check the number of duplicate observations\n","print(\"Duplicated observations: \" + df.duplicated().sum().astype(str))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zoBBbzA9QWz8"},"outputs":[],"source":["# Check the number of unique values ​​of each data field\n","df.nunique()"]},{"cell_type":"markdown","metadata":{"id":"MtKlPj7aRCg5"},"source":["### 2.2.2. Check the fields in the dataset"]},{"cell_type":"markdown","metadata":{"id":"OW-4F01oTI-n"},"source":["####2.2.2.1. Descriptive Analysis\n"]},{"cell_type":"markdown","metadata":{"id":"LF4nbFUSu_OF"},"source":["- Descriptive Statistics data fields in a data set\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wnPNaiPES975"},"outputs":[],"source":["df.describe()"]},{"cell_type":"markdown","metadata":{"id":"JCc3k2TsuqNU"},"source":["The authors examine the dataset and gain insights into the \"Units Sold\" variable. On average, 4528 units are sold, with a significant variation indicated by a standard deviation of 2839 units.\n","\n","To better understand the distribution of units sold, the authors look at the quartiles. The median number of units sold is 4222.5, showing that half of the sales are below this amount while the other half exceeds it. Notably, there are outliers, with units sold ranging from a minimum of 1 to a maximum of 10000. These extremes may result from specific company circumstances, highlighting the diverse issues that businesses face."]},{"cell_type":"markdown","metadata":{"id":"bbGKhn37ZU5X"},"source":["### 2.2.3. Explore data fields"]},{"cell_type":"markdown","metadata":{"id":"ioROBCbRezqp"},"source":["#### 2.2.3.1. Explore stock price data fields"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a7bIEOFde_Hd"},"outputs":[],"source":["dfl = pd.melt(df[['Month-Year', 'Units Sold']], ['Month-Year'])\n","\n","plt.figure(figsize=(16, 6))\n","sns.lineplot(data = dfl,\n","             x = 'Month-Year',\n","             y = 'value',\n","             hue = 'variable')\n","plt.title('Units Sold Over Time')\n","plt.xlabel('Date')\n","plt.ylabel('Stock Price')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"yasST-dFm1Ld"},"source":["Based on the chart above, we can see that revenue fluctuated a lot from 2010 to 2017, and there must be many factors affecting this fluctuation. Let's analyze this fluctuation more closely and build a model to evaluate and predict the total units sold of the company.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fFiOAxGAn2a0"},"source":["####2.2.3.2: Univariate Analysis"]},{"cell_type":"markdown","metadata":{"id":"EepoBoqxn3Tj"},"source":["In this section, our group will show the distribution chart of the four continuous variables : Total Revenue, Total Cost, Total Profit, Gross Profit Margin."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"91lRhxl5gM0C"},"outputs":[],"source":["features = ['Total Revenue', 'Total Cost', 'Total Profit', 'Gross Profit Margin']\n","\n","fig, axes = plt.subplots(1, 4, figsize=(16, 6))\n","for i, feature in enumerate(features):\n","    sns.histplot(df[feature], kde=True, ax=axes[i])\n","    axes[i].set_title(f'Distribution of {feature}')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"mb6IlNZkkPfk"},"source":["The histograms reveal diverse financial landscapes within the dataset. Total revenue, total cost, and total profit show right-skewed distributions, indicating a concentration of lower values with a few outliers exhibiting exceptionally high figures. This suggests variability in company size and performance. Gross profit margin, however, displays a bimodal distribution with two distinct peaks, hinting at the existence of two distinct groups with differing profitability characteristics.|"]},{"cell_type":"markdown","metadata":{"id":"LwS9BGXhsSNu"},"source":["####2.2.3.3. Bivariate Analysis"]},{"cell_type":"code","source":["# Select only numerical columns\n","numerical_columns = df.select_dtypes(include=[np.number])\n","\n","# Drop 'Month-Year' column if it's still present (assuming it's datetime or string type)\n","if 'Month-Year' in numerical_columns.columns:\n","    numerical_columns = numerical_columns.drop(columns=['Month-Year'])\n","\n","# Generate the correlation matrix\n","correlation_matrix = numerical_columns.corr()\n","\n","# Plot the heatmap\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n","plt.title('Correlation Matrix Heatmap')\n","plt.show()\n"],"metadata":{"id":"B_SNrXJ_igws"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B83wlnc_uM9t"},"source":["The relationship between the variables on the heatmap with positive correlation to Units Sold can be understood as follows:\n","\n","- Units Sold and Total Revenue: The correlation coefficient is 0.43, indicating a weak relationship between units sold and total revenue. This suggests that while there is some association, the number of units sold is not the main factor driving total revenue.\n","\n","- Units Sold and Total Cost: The correlation coefficient is 0.32, indicating a weak positive relationship. This suggests that an increase in units sold is slightly associated with an increase in total cost, but the relationship is not strong.\n","\n","- Units Sold and Total Profit: The correlation coefficient is 0.56, indicating a moderate positive relationship. This suggests that as units sold increase, total profit also tends to increase, but other factors may also play a significant role.\n","\n","- Units Sold and Gross Profit Margin: The correlation coefficient is 0.16, indicating a very weak positive relationship. This suggests that units sold have little to no impact on the gross profit margin.\n","\n","- Units Sold and ROI: The correlation coefficient is 0.11, indicating a very weak positive relationship. This suggests that units sold have little to no impact on the return on investment (ROI).\n","\n","Overall, the low correlation coefficients indicate that there is no strong relationship between these variables and units sold. In business analysis, this can be understood to mean that factors such as total revenue, total cost, total profit, gross profit margin, and ROI do not greatly influence the number of units sold. This may also reflect the complex nature of business dynamics, where other factors aside from these metrics play significant roles in influencing sales performance."]},{"cell_type":"markdown","metadata":{"id":"BqGm8BfB481x"},"source":["# 3. Model Building and Evalutions"]},{"cell_type":"markdown","source":["## 3.1. K-Means Clustering"],"metadata":{"id":"rEP5zvnkfMBD"}},{"cell_type":"code","source":["data=pd.read_csv('/content/drive/MyDrive/[H4TF DA] Round 2_MLTada/data for kmean.csv')\n","data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"uBkurOHci2xA","executionInfo":{"status":"ok","timestamp":1719232142412,"user_tz":-420,"elapsed":1358,"user":{"displayName":"James Valix","userId":"10011400396215233588"}},"outputId":"28ea3431-cd63-420f-b5d8-896a683e1e36"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    order_id  shipping_time  units_sold  total_revenue\n","0  753534924             26        6584       28785248\n","1  431350123              8        4942      126159376\n","2  737043845              6        6507       71108496\n","3  944893586             12        3865      258286355\n","4  959338033              7        8297      350042133"],"text/html":["\n","  <div id=\"df-2555d39d-3a9c-4d75-a041-7eaae994ae3b\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>order_id</th>\n","      <th>shipping_time</th>\n","      <th>units_sold</th>\n","      <th>total_revenue</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>753534924</td>\n","      <td>26</td>\n","      <td>6584</td>\n","      <td>28785248</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>431350123</td>\n","      <td>8</td>\n","      <td>4942</td>\n","      <td>126159376</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>737043845</td>\n","      <td>6</td>\n","      <td>6507</td>\n","      <td>71108496</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>944893586</td>\n","      <td>12</td>\n","      <td>3865</td>\n","      <td>258286355</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>959338033</td>\n","      <td>7</td>\n","      <td>8297</td>\n","      <td>350042133</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2555d39d-3a9c-4d75-a041-7eaae994ae3b')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-2555d39d-3a9c-4d75-a041-7eaae994ae3b button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-2555d39d-3a9c-4d75-a041-7eaae994ae3b');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-5e86c750-89b2-4c2f-bba2-0219fa5888eb\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5e86c750-89b2-4c2f-bba2-0219fa5888eb')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-5e86c750-89b2-4c2f-bba2-0219fa5888eb button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"data","summary":"{\n  \"name\": \"data\",\n  \"rows\": 100000,\n  \"fields\": [\n    {\n      \"column\": \"order_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 259321871,\n        \"min\": 100008904,\n        \"max\": 999996459,\n        \"num_unique_values\": 100000,\n        \"samples\": [\n          718163669,\n          659689795,\n          984582126\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"shipping_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14,\n        \"min\": 0,\n        \"max\": 50,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          41,\n          31,\n          27\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"units_sold\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2884,\n        \"min\": 1,\n        \"max\": 10000,\n        \"num_unique_values\": 9998,\n        \"samples\": [\n          9643,\n          7259,\n          3170\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_revenue\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 141824330,\n        \"min\": 933,\n        \"max\": 668069519,\n        \"num_unique_values\": 65639,\n        \"samples\": [\n          146782734,\n          126855708,\n          11926486\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["sns.boxplot(x=data['units_sold'])\n","plt.show()"],"metadata":{"id":"sow5Jq25orsj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After building the boxplot of Units_Sold we see that the Units_Sold column has no Outliers."],"metadata":{"id":"FlbWgtGOzDEi"}},{"cell_type":"markdown","source":["Scale data (also known as feature scaling) is the process of normalizing data to bring them to the same value range. Typically, scaling data helps improve the performance of machine learning and data analysis algorithms. This is a very important step in building Model K-means Clustering"],"metadata":{"id":"hJdP771czzFJ"}},{"cell_type":"code","source":["data_kmean = data.copy()\n","cols_del = ['order_id']\n","data_kmean = data_kmean.drop(cols_del, axis=1)\n","scaler = StandardScaler()\n","scaler.fit(data_kmean)\n","scaled_data = pd.DataFrame(scaler.transform(data_kmean),columns= data_kmean.columns )\n","print(\"All features are now scaled\")"],"metadata":{"id":"bSBc6kkzotSo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scaled_data.head()"],"metadata":{"id":"PNrD8lygoxHY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The Elbow method is one of the popular methods to determine the optimal value of k in the K-Means clustering algorithm. This is a heuristics method used to determine the number of clusters in a data set."],"metadata":{"id":"oav9DWAfz2Wv"}},{"cell_type":"code","source":["kmeans = KMeans(random_state=0)\n","print('Elbow Method to determine the number of clusters to be formed:')\n","Elbow_M = KElbowVisualizer(KMeans(), k=10)\n","Elbow_M.fit(scaled_data)\n","Elbow_M.show()"],"metadata":{"id":"I_ourG_voyRP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Using the Elbow Method, we were able to determine the most optimal number of clusters would be '5'."],"metadata":{"id":"LqELiPklws7A"}},{"cell_type":"code","source":["#Set random state for Model\n","random_state = 8\n","# Building K-means Clustering Model\n","k = 5\n","\n","kmeans = KMeans(n_clusters=k, random_state=42)\n","kmeans.fit(scaled_data)\n","\n","data_kmean['Cluster'] = kmeans.labels_\n","\n","print(data_kmean.head(15))\n","print(data_kmean['Cluster'].unique())"],"metadata":{"id":"OLwd5R5LoyPF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After our team chooses the appropriate Cluster parameters for the Model, the team proceeds to build the Model with the metrics Shipping Time, Units Sold and Total Revenue and devide Customer into 5 groups."],"metadata":{"id":"5vl4vqiw0t7C"}},{"cell_type":"code","source":["data['clusters'] = kmeans.labels_"],"metadata":{"id":"kf02hvLPoyLX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.head()"],"metadata":{"id":"a5UjvfocoyJn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Distribution cluster\n","sns.countplot(x='clusters', data=data)\n","plt.show()"],"metadata":{"id":"d9kkS7W5oyHX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After classifying customers, we can see that group number 2 is a very small group and the largest group is group number 4. Let's analyze each group more closely in the next section."],"metadata":{"id":"QafAGaq31lbP"}},{"cell_type":"code","source":["data.groupby(by='clusters').mean()"],"metadata":{"id":"XVKWVtuLoxvn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n","\n","for i in range(0, 5):\n","    sns.histplot(data[data['clusters'] == i]['units_sold'], ax=axes[i])\n","    axes[i].set_title('Cluster ' + str(i))\n","    axes[i].set_xlabel('Units Sold')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"aq0pk_Ito5Tw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n","\n","for i in range(0, 5):\n","    sns.histplot(data[data['clusters'] == i]['shipping_time'], ax=axes[i])\n","    axes[i].set_title('Cluster ' + str(i))\n","    axes[i].set_xlabel('Shipping Time')\n","    axes[i].set_ylabel('Frequency')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"LP-N7h-Go5uo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n","\n","for i in range(0, 5):\n","    sns.histplot(data[data['clusters'] == i]['total_revenue'], ax=axes[i])\n","    axes[i].set_title('Cluster ' + str(i))\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"_Y2uu9KzpCKg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The team chose the following names for the five customer groups after examining their attributes:\n","- Economical Occasional Buyers: This group purchases relatively little, generates little revenue, and experiences extremely slow shipping.\n","- High-Spending Patient Buyers: This customer segment has a medium spending level but a slow shipping time.\n","- High-Spending Impulse Buyers: Members of this group tend to make large, impulsive purchases, but their shipping times are erratic.\n","- Economical Fast Buyers: These are clients who spend little money and make modest purchases, but they have quick shipping.\n","- Medium-Spending Efficient Buyers: These are clients who make very large purchases, have a medium level of spending, and have very long shipping times."],"metadata":{"id":"w5vMJXhi2BXO"}},{"cell_type":"markdown","metadata":{"id":"My45P8Ss_UFX"},"source":["## 3.2. LSTM Model"]},{"cell_type":"markdown","metadata":{"id":"voBRbLWUO6aX"},"source":["## Model Preprocessing\n"]},{"cell_type":"markdown","metadata":{"id":"ckLjGPrIPAK2"},"source":["The model is preprocessed in these particular steps:\n","* Create a copy of the original data\n","* Sort the data by `Month-Year` for easy appending and splitting operations later on\n","* Select only the `Month-Year` and `Units Sold` columns as these are the columns we are interested in predicting\n","* Converting the DataFrame into a numpy array to fit within the LSTM model"]},{"cell_type":"markdown","metadata":{"id":"-xvO60sPPD9i"},"source":["As we are interested in building a model that can predict stock prices even in the circumstance that extreme external conditions are affecting the dataset, we decide to keep the whole set of data for prediction. Since although regular fluctuations can be seen throughout the period, the level of fluctuation and periodity remains the same. Future implementations might make a model with recent data seems more beneficial, but strict record in anomalies and data disruption might be required to determine that."]},{"cell_type":"markdown","metadata":{"id":"hb_1oJ8BPLuZ"},"source":["The data will then be split into `x_train` and `y_train` with the idea that for any values $i$ in `y_train`, the model will use values from $i - 1$ to $i - size$ to predict that particular value, or a sliding window approach.\n","\n","The data is then converted into numpy arrays and reshaped to fit into the LSTM model."]},{"cell_type":"markdown","metadata":{"id":"y4j-FiW3PQlT"},"source":["## Model Training\n"]},{"cell_type":"markdown","metadata":{"id":"ZcwMWrPzPTDd"},"source":["We have tuned the data based on multiple rounds of trial and errors and we have decided on these hyperparameters:\n","* 2 layers model: The data is complex instead of exihibiting a stable trend so we will need 2 layers instead of 1.the\n","* `batch_size = 32`: This is generally considered  default value for `batch_size`.\n","* `optimizer = 'rmsprop'`: `rmsprop` is a well-known and widely used optimizer.\n","* `loss = 'mean_squared_error'`: This is used as MRSE is one of the criteria required by Business Case to evaluate the two models"]},{"cell_type":"markdown","metadata":{"id":"yeEmDvPn_h_V"},"source":["## Model Testing"]},{"cell_type":"markdown","metadata":{"id":"WDOgAiMrPbja"},"source":["The test dataset is extracted and processed in a similar fashion as the traning\n"]},{"cell_type":"markdown","metadata":{"id":"NngjSwi5_lja"},"source":["## Model Evaluation"]},{"cell_type":"markdown","metadata":{"id":"Ha2_6YHiPe2P"},"source":["Inheriting the requirements from the Business Case 1 prompt, this model will be evaluated based on 3 metricsL: R-squared, MAPE, and RMSE. The team has calculated these values for this model."]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import r2_score\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM, Dropout\n","import matplotlib.pyplot as plt\n","\n","\n","df_grouped = df.groupby('Month-Year')['Units Sold'].sum().reset_index() #group by sum of units sold per month-year\n","df_grouped['Month-Year'] = pd.to_datetime(df_grouped['Month-Year']) #convert month-year to datetime\n","df_grouped = df_grouped.sort_values(by=['Month-Year'])\n","#Set random state for Model\n","random_state = 8\n","# Filter out only 'Units Sold' column for the LSTM model and convert the DataFrame to a numpy array\n","HPG_subset = df_grouped[['Units Sold']]\n","HPG_LSTM = HPG_subset.values\n","\n","# Train-Test Split\n","total_len = len(HPG_LSTM)\n","train_ratio = 0.8  # 80 Train - 20 Test\n","training_data_len = int(total_len * train_ratio)\n","\n","# MinMaxScaler()\n","MMS = MinMaxScaler()\n","scaled_data = MMS.fit_transform(HPG_LSTM)\n","\n","# Create the scaled training dataset\n","train_data = scaled_data[0:training_data_len, :]\n","\n","x_train = []\n","y_train = []\n","size = 15\n","\n","for i in range(size, len(train_data)):\n","    x_train.append(train_data[i-size:i, 0])\n","    y_train.append(train_data[i, 0])\n","\n","x_train, y_train = np.array(x_train), np.array(y_train)\n","x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n","\n","      # <LSTM model>\n","model = Sequential()\n","model.add(LSTM(128, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n","model.add(Dropout(0.2))  # Dropout layer to prevent overfitting\n","model.add(Bidirectional(LSTM(64, return_sequences=False)))\n","model.add(Dense(25))\n","model.add(Dense(1))\n","\n","# Model compile and fit\n","model.compile(optimizer='rmsprop', loss='mean_squared_error')\n","model.fit(x_train, y_train, batch_size=32, epochs=30)\n","\n","# Create the test dataset\n","test_data = scaled_data[training_data_len - size:, :]\n","\n","# Split the data sets x_test and y_test\n","x_test = []\n","y_test = HPG_LSTM[training_data_len:, :]\n","for i in range(size, len(test_data)):\n","    x_test.append(test_data[i-size:i, 0])\n","\n","x_test = np.array(x_test)\n","x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n","\n","# Predictions\n","predictions = model.predict(x_test)\n","predictions = MMS.inverse_transform(predictions)\n","\n","# Metrics\n","r_squared = r2_score(y_test, predictions)\n","def mean_absolute_percentage_error(y_true, y_pred):\n","    y_true, y_pred = np.array(y_true), np.array(y_pred)\n","    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","\n","mape = mean_absolute_percentage_error(y_test, predictions)\n","rmse = np.sqrt(np.mean((predictions - y_test) ** 2))\n","\n","# Print metrics\n","LSTM_metrics = pd.DataFrame({\n","    'R-squared': [r_squared],\n","    'MAPE': [mape],\n","    'RMSE': [rmse]\n","}, index=['LSTM'])\n","\n","print(LSTM_metrics)\n","\n","# Plotting\n","viz_train = df_grouped[:training_data_len].copy()\n","viz_train.set_index('Month-Year', inplace=True)\n","\n","viz_test = df_grouped[training_data_len:].copy()\n","viz_test.set_index('Month-Year', inplace=True)\n","viz_test.insert(1, 'Predictions', predictions, True)\n","\n","# Predicting data in 1 year\n","n_steps = 12  # 12Months\n","x_input = train_data[-size:].reshape((1, size, 1))\n","future_predictions = []\n","\n","for i in range(n_steps):\n","    yhat = model.predict(x_input, verbose=0)\n","    future_predictions.append(yhat[0, 0])\n","    x_input = np.roll(x_input, -1)\n","    x_input[0, (size-1), 0] = yhat\n","\n","future_predictions = MMS.inverse_transform(np.array(future_predictions).reshape(-1, 1))\n","future_dates = pd.date_range(start=viz_test.index[-1], periods=n_steps+1, freq='M')[1:]\n","future_df = pd.DataFrame(future_predictions, index=future_dates, columns=['Future Predictions'])\n","\n","plt.figure(figsize=(16, 6))\n","plt.plot(viz_train.index, viz_train['Units Sold'], label='Train')\n","plt.plot(viz_test.index, viz_test['Units Sold'], label='Test')\n","plt.plot(future_df.index, future_df['Future Predictions'], label='Future Predictions')\n","plt.title('Units Sold Prediction with LSTM Model')\n","plt.xlabel('Time')\n","plt.ylabel('Units Sold')\n","plt.legend(loc='upper left')\n","plt.show()"],"metadata":{"id":"Xa-ZEQglWFX0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Metrics Analysis**\n","- R-Squared (-0.087): On a scale of 1, a negative value would determine that the model is not doing a great job in predicting accurately.\n","- MAPE (2.8): A MAPE of 2.80% indicates that, on average, the model's predictions are off by 2.80%. This is a relatively low error rate and could be considered good\n","- RMSE (147130.95): A substantially high number for the Root Mean Square Error of the prediction comparing to actual data, indicating an inaccurate prediction."],"metadata":{"id":"wBsf3Adp6rMR"}},{"cell_type":"markdown","source":["**Model Optimization**\n","The provided metrics are, although indicative of the model’s malfunctioning, this shed light for further optimization techniques for this model.\n"," - Feature Engineering: Creating lagged variables to capture temporal dependencies, generating moving averages to identify trends, adding categorical features to account for cyclical patterns, and scaling or normalizing features to ensure they are on the same scale.\n"," - Different Algorithms: Algorithms such as Random Forests and Gradient Boosting can capture non-linear relationships and interactions between features. Support Vector Machines (SVMs) are effective for high-dimensional spaces and non-linear classification, while neural networks, including architectures like GRU or convolutional neural networks (CNNs), can be powerful for time series data.\n"," - Hyperparameter Tuning: Techniques such as grid search and random search systematically explore the hyperparameter space to find the best combination. Bayesian optimization offers a more efficient approach by using probabilistic models to balance exploration and exploitation. Cross-validation ensures that the tuned hyperparameters generalize well to unseen data."],"metadata":{"id":"gltGVYcW610s"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}